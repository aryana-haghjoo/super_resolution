{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening a random .fits file from Unzipped JADES spectra to see what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/aryanahaghjoo/Documents/GitHub/super_resolution/data/JADES/JADES_spectra_unzipped/hlsp_jades_jwst_nirspec_clear-prism/hlsp_jades_jwst_nirspec_goods-n-mediumhst-00000604_clear-prism_v1.0_s2d.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      34   ()      \n",
      "  1  FLUX          1 ImageHDU         9   (674, 27)   float64   \n",
      "  2  FLUX_ERR      1 ImageHDU         9   (674, 27)   float64   \n",
      "  3  WAVELENGTH    1 ImageHDU         8   (674,)   float64   \n",
      "  4  RA            1 ImageHDU         9   (674, 27)   float64   \n",
      "  5  DEC           1 ImageHDU         9   (674, 27)   float64   \n",
      "  6  ASDF          1 BinTableHDU      9   0R x 0C   []   \n",
      "\n",
      "Header of the Primary HDU:\n",
      "SIMPLE  =                    T / conforms to FITS standard                      \n",
      "BITPIX  =                    8 / array data type                                \n",
      "NAXIS   =                    0 / number of array dimensions                     \n",
      "EXTEND  =                    T                                                  \n",
      "BUNIT   = 'erg s-1 cm-2 AA-1'                                                   \n",
      "RA      =    189.1172020940793 / Target RA in the APT catalogue                 \n",
      "DEC     =    62.22143369975083 / Target DEC in the APT catalogue                \n",
      "TARGET  = '00000604'           / the observation target                         \n",
      "XPOSURE =            6127.3338 / [s] effective integration time                 \n",
      "DATE-BEG= '2023-02-07T07:52:53.381' / Date-time start of exposure               \n",
      "DATE-END= '2023-02-07T09:48:08.610' / Date-time end of exposure                 \n",
      "DOI     = '10.17909/8tdj-8n28'                                                  \n",
      "RADESYS = 'ICRS    '                                                            \n",
      "HLSPID  = 'JADES   '                                                            \n",
      "HLSPLEAD= 'Marcia Rieke'                                                        \n",
      "HLSPNAME= 'JWST Advanced Deep Extragalactic Survey'                             \n",
      "HLSPTARG= 'GOODS-North-Medium' / PID                                            \n",
      "HLSPTIER= 'medium_hst_gn'      / Tier name                                      \n",
      "HLSPVER = '1.0     '                                                            \n",
      "INSTRUME= 'NIRSpec '                                                            \n",
      "LICENSE = 'CC BY 4.0'                                                           \n",
      "LICENURL= 'https://creativecommons.org/licenses/by/4.0/'                        \n",
      "PROPOSID=                 1181 / Proposal ID                                    \n",
      "TELAPSE =  0.08003737268518663 / [d]                                            \n",
      "TIMESYS = 'UTC     '                                                            \n",
      "MODE    = 'MOS     '                                                            \n",
      "APERTURE= '1x3     '                                                            \n",
      "APERFH  = '5       '                                                            \n",
      "FILTER  = 'CLEAR   '                                                            \n",
      "GRATING = 'PRISM   '                                                            \n",
      "MJD-BEG =    59982.32839561343 / [d] exposure start time in MJD                 \n",
      "MJD-END =    59982.32839561343 / [d] exposure end time in MJD                   \n",
      "MJD-MID =    59982.36841429977 / [d] exposure mid time in MJD                   \n",
      "TELESCOP= 'JWST    '                                                            \n",
      "\n",
      "Header of Extension HDU:\n",
      "XTENSION= 'IMAGE   '           / Image extension                                \n",
      "BITPIX  =                  -64 / array data type                                \n",
      "NAXIS   =                    2 / number of array dimensions                     \n",
      "NAXIS1  =                  674                                                  \n",
      "NAXIS2  =                   27                                                  \n",
      "PCOUNT  =                    0 / number of parameters                           \n",
      "GCOUNT  =                    1 / number of groups                               \n",
      "EXTNAME = 'FLUX    '                                                            \n",
      "GUNIT   = 'erg s-1 cm-2 AA-1'                                                   \n",
      "\n",
      "Data in Extension HDU (First 5 Rows):\n",
      "[[            nan -4.52214370e-21 -4.83914586e-21 ...             nan\n",
      "              nan             nan]\n",
      " [            nan  8.18968075e-21  3.31638870e-21 ...             nan\n",
      "              nan             nan]\n",
      " [            nan  3.52141988e-22 -6.57508299e-21 ...             nan\n",
      "              nan             nan]\n",
      " [            nan -3.05642117e-21 -9.96892583e-21 ...             nan\n",
      "              nan             nan]\n",
      " [            nan  3.59288271e-21 -7.54090071e-22 ...             nan\n",
      "              nan             nan]]\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your FITS file\n",
    "fits_file = '/Users/aryanahaghjoo/Documents/GitHub/super_resolution/data/JADES/JADES_spectra_unzipped/hlsp_jades_jwst_nirspec_clear-prism/hlsp_jades_jwst_nirspec_goods-n-mediumhst-00000604_clear-prism_v1.0_s2d.fits'\n",
    "\n",
    "# Open the FITS file\n",
    "with fits.open(fits_file) as hdul:\n",
    "    # Print the FITS file structure\n",
    "    hdul.info()\n",
    "    \n",
    "    # Access the primary HDU (Header Data Unit)\n",
    "    primary_hdu = hdul[0]\n",
    "    \n",
    "    # Print the header\n",
    "    print(\"\\nHeader of the Primary HDU:\")\n",
    "    print(repr(primary_hdu.header))\n",
    "    \n",
    "    # Access the data (if it's an image)\n",
    "    if primary_hdu.data is not None:\n",
    "        print(\"\\nData shape:\", primary_hdu.data.shape)\n",
    "\n",
    "    # If there's an extension (e.g., table), access it\n",
    "    if len(hdul) > 1:\n",
    "        table_hdu = hdul[1]\n",
    "        print(\"\\nHeader of Extension HDU:\")\n",
    "        print(repr(table_hdu.header))\n",
    "        print(\"\\nData in Extension HDU (First 5 Rows):\")\n",
    "        print(table_hdu.data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extracting Spectrum from \"prism\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|██████████| 7504/7504 [00:08<00:00, 893.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your FITS files with 1D spectra\n",
    "fits_dir = '/Users/aryanahaghjoo/Documents/GitHub/super_resolution/data/JADES/JADES_spectra_unzipped/hlsp_jades_jwst_nirspec_clear-prism'\n",
    "# Collect both lowercase and uppercase file extensions\n",
    "fits_files = glob.glob(os.path.join(fits_dir, '*.fits')) + glob.glob(os.path.join(fits_dir, '*.FITS'))\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in tqdm(fits_files, desc=\"Processing FITS files\"):\n",
    "    with fits.open(file) as hdul:\n",
    "        # Search for the EXTRACT1D extension\n",
    "        extract1d_hdu = None\n",
    "        for hdu in hdul:\n",
    "            if hdu.header.get('EXTNAME', '').strip().upper() == 'EXTRACT1D':\n",
    "                extract1d_hdu = hdu\n",
    "                break\n",
    "\n",
    "        # Skip files that are not 1D spectra\n",
    "        if extract1d_hdu is None:\n",
    "            continue\n",
    "\n",
    "        # Confirm the extension is a table with columns\n",
    "        if not hasattr(extract1d_hdu, 'columns'):\n",
    "            print(f\"Warning: 'EXTRACT1D' in {os.path.basename(file)} is not a table. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        table_data = extract1d_hdu.data\n",
    "        available_columns = [col.upper() for col in extract1d_hdu.columns.names]\n",
    "        required_columns = ['WAVELENGTH', 'FLUX', 'FLUX_ERR']\n",
    "        if not all(col in available_columns for col in required_columns):\n",
    "            print(f\"Warning: Missing required columns in {os.path.basename(file)}. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        # Extract data columns\n",
    "        wavelength = table_data['WAVELENGTH']\n",
    "        flux = table_data['FLUX']\n",
    "        flux_err = table_data['FLUX_ERR']\n",
    "\n",
    "        data_list.append({\n",
    "            'file_name': os.path.basename(file),\n",
    "            'WAVELENGTH': wavelength,\n",
    "            'FLUX': flux,\n",
    "            'FLUX_ERR': flux_err\n",
    "        })\n",
    "\n",
    "# Create a DataFrame where each row corresponds to a file\n",
    "df_prism= pd.DataFrame(data_list)\n",
    "#df_prism.to_csv('/Users/aryanahaghjoo/Documents/GitHub/super_resolution/toy_model/JADES_spectra_dataframe/prism.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_nan_positions(arr):\n",
    "    \"\"\"\n",
    "    Classify NaN positions in a 1D numpy array.\n",
    "\n",
    "    Returns:\n",
    "      - 'no_nans' if there are no NaN values.\n",
    "      - 'all_nans' if the entire array is NaN.\n",
    "      - 'trailing_nans' if all NaNs occur only after the last valid (non-NaN) value.\n",
    "      - 'middle_nans' if any NaN occurs before the last valid value.\n",
    "    \"\"\"\n",
    "    nan_indices = np.where(np.isnan(arr))[0]\n",
    "    if len(nan_indices) == 0:\n",
    "        return 'no_nans'\n",
    "    \n",
    "    valid_indices = np.where(~np.isnan(arr))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        return 'all_nans'\n",
    "    \n",
    "    last_valid = valid_indices[-1]\n",
    "    # If all NaN indices are strictly greater than the last valid index,\n",
    "    # then NaNs are only trailing.\n",
    "    if np.all(nan_indices > last_valid):\n",
    "        return 'trailing_nans'\n",
    "    else:\n",
    "        return 'middle_nans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs in FLUX: 28487\n",
      "Total NaNs in FLUX_ERR: 28487\n",
      "NaN positions in FLUX:\n",
      "flux_nan_position\n",
      "middle_nans      3028\n",
      "trailing_nans     724\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NaN positions in FLUX_ERR:\n",
      "flux_err_nan_position\n",
      "middle_nans      3028\n",
      "trailing_nans     724\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count NaN entries in each file's FLUX and FLUX_ERR arrays\n",
    "df_prism['nan_in_flux'] = df_prism['FLUX'].apply(lambda arr: np.isnan(arr).sum())\n",
    "df_prism['nan_in_flux_err'] = df_prism['FLUX_ERR'].apply(lambda arr: np.isnan(arr).sum())\n",
    "\n",
    "# Display the count per file\n",
    "#print(df_prism[['file_name', 'nan_in_flux', 'nan_in_flux_err']])\n",
    "\n",
    "# Sum across all files to get the total number of NaN entries\n",
    "total_nan_flux = df_prism['nan_in_flux'].sum()\n",
    "total_nan_flux_err = df_prism['nan_in_flux_err'].sum()\n",
    "\n",
    "print(\"Total NaNs in FLUX:\", total_nan_flux)\n",
    "print(\"Total NaNs in FLUX_ERR:\", total_nan_flux_err)\n",
    "\n",
    "# Apply the classification to your DataFrame columns:\n",
    "df_prism['flux_nan_position'] = df_prism['FLUX'].apply(classify_nan_positions)\n",
    "df_prism['flux_err_nan_position'] = df_prism['FLUX_ERR'].apply(classify_nan_positions)\n",
    "\n",
    "# Optionally, count the occurrences in each category:\n",
    "flux_counts = df_prism['flux_nan_position'].value_counts()\n",
    "flux_err_counts = df_prism['flux_err_nan_position'].value_counts()\n",
    "\n",
    "print(\"NaN positions in FLUX:\")\n",
    "print(flux_counts)\n",
    "print(\"\\nNaN positions in FLUX_ERR:\")\n",
    "print(flux_err_counts)\n",
    "\n",
    "# You can also separate the files based on classification, for example:\n",
    "df_prism_middle = df_prism[(df_prism['flux_nan_position'] == 'middle_nans') | (df_prism['flux_err_nan_position'] == 'middle_nans')]\n",
    "df_prism_trailing = df_prism[(df_prism['flux_nan_position'] == 'trailing_nans') & (df_prism['flux_err_nan_position'] == 'trailing_nans')]\n",
    "\n",
    "#print(\"\\nFiles with NaNs in the middle:\")\n",
    "#print(df_prism_middle[['file_name', 'flux_nan_position', 'flux_err_nan_position']])\n",
    "#print(\"\\nFiles with only trailing NaNs:\")\n",
    "#print(df_prism_trailing[['file_name', 'flux_nan_position', 'flux_err_nan_position']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Spectrum from \"g140m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|██████████| 6968/6968 [00:08<00:00, 790.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your FITS files with 1D spectra\n",
    "fits_dir = '/Users/aryanahaghjoo/Documents/GitHub/super_resolution/data/JADES/JADES_spectra_unzipped/hlsp_jades_jwst_nirspec_f070lp-g140m'\n",
    "# Collect both lowercase and uppercase file extensions\n",
    "fits_files = glob.glob(os.path.join(fits_dir, '*.fits')) + glob.glob(os.path.join(fits_dir, '*.FITS'))\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in tqdm(fits_files, desc=\"Processing FITS files\"):\n",
    "    with fits.open(file) as hdul:\n",
    "        # Search for the EXTRACT1D extension\n",
    "        extract1d_hdu = None\n",
    "        for hdu in hdul:\n",
    "            if hdu.header.get('EXTNAME', '').strip().upper() == 'EXTRACT1D':\n",
    "                extract1d_hdu = hdu\n",
    "                break\n",
    "\n",
    "        # Skip files that are not 1D spectra\n",
    "        if extract1d_hdu is None:\n",
    "            continue\n",
    "\n",
    "        # Confirm the extension is a table with columns\n",
    "        if not hasattr(extract1d_hdu, 'columns'):\n",
    "            print(f\"Warning: 'EXTRACT1D' in {os.path.basename(file)} is not a table. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        table_data = extract1d_hdu.data\n",
    "        available_columns = [col.upper() for col in extract1d_hdu.columns.names]\n",
    "        required_columns = ['WAVELENGTH', 'FLUX', 'FLUX_ERR']\n",
    "        if not all(col in available_columns for col in required_columns):\n",
    "            print(f\"Warning: Missing required columns in {os.path.basename(file)}. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        # Extract data columns\n",
    "        wavelength = table_data['WAVELENGTH']\n",
    "        flux = table_data['FLUX']\n",
    "        flux_err = table_data['FLUX_ERR']\n",
    "\n",
    "        data_list.append({\n",
    "            'file_name': os.path.basename(file),\n",
    "            'WAVELENGTH': wavelength,\n",
    "            'FLUX': flux,\n",
    "            'FLUX_ERR': flux_err\n",
    "        })\n",
    "\n",
    "# Create a DataFrame where each row corresponds to a file\n",
    "df_g140m= pd.DataFrame(data_list)\n",
    "#df_g140m.to_csv('/Users/aryanahaghjoo/Documents/GitHub/super_resolution/toy_model/JADES_spectra_dataframe/g140m.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs in FLUX: 382330\n",
      "Total NaNs in FLUX_ERR: 382330\n",
      "NaN positions in FLUX:\n",
      "flux_nan_position\n",
      "middle_nans      3414\n",
      "trailing_nans      59\n",
      "no_nans            11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NaN positions in FLUX_ERR:\n",
      "flux_err_nan_position\n",
      "middle_nans      3414\n",
      "trailing_nans      59\n",
      "no_nans            11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count NaN entries in each file's FLUX and FLUX_ERR arrays\n",
    "df_g140m['nan_in_flux'] = df_g140m['FLUX'].apply(lambda arr: np.isnan(arr).sum())\n",
    "df_g140m['nan_in_flux_err'] = df_g140m['FLUX_ERR'].apply(lambda arr: np.isnan(arr).sum())\n",
    "\n",
    "# Display the count per file\n",
    "#print(df_prism[['file_name', 'nan_in_flux', 'nan_in_flux_err']])\n",
    "\n",
    "# Sum across all files to get the total number of NaN entries\n",
    "total_nan_flux = df_g140m['nan_in_flux'].sum()\n",
    "total_nan_flux_err = df_g140m['nan_in_flux_err'].sum()\n",
    "\n",
    "print(\"Total NaNs in FLUX:\", total_nan_flux)\n",
    "print(\"Total NaNs in FLUX_ERR:\", total_nan_flux_err)\n",
    "\n",
    "# Apply the classification to your DataFrame columns:\n",
    "df_g140m['flux_nan_position'] = df_g140m['FLUX'].apply(classify_nan_positions)\n",
    "df_g140m['flux_err_nan_position'] = df_g140m['FLUX_ERR'].apply(classify_nan_positions)\n",
    "\n",
    "# Optionally, count the occurrences in each category:\n",
    "flux_counts = df_g140m['flux_nan_position'].value_counts()\n",
    "flux_err_counts = df_g140m['flux_err_nan_position'].value_counts()\n",
    "\n",
    "print(\"NaN positions in FLUX:\")\n",
    "print(flux_counts)\n",
    "print(\"\\nNaN positions in FLUX_ERR:\")\n",
    "print(flux_err_counts)\n",
    "\n",
    "# You can also separate the files based on classification, for example:\n",
    "df_g140m_middle = df_g140m[(df_g140m['flux_nan_position'] == 'middle_nans') | (df_g140m['flux_err_nan_position'] == 'middle_nans')]\n",
    "df_g140m_trailing = df_g140m[(df_g140m['flux_nan_position'] == 'trailing_nans') & (df_g140m['flux_err_nan_position'] == 'trailing_nans')]\n",
    "\n",
    "#print(\"\\nFiles with NaNs in the middle:\")\n",
    "#print(df_prism_middle[['file_name', 'flux_nan_position', 'flux_err_nan_position']])\n",
    "#print(\"\\nFiles with only trailing NaNs:\")\n",
    "#print(df_prism_trailing[['file_name', 'flux_nan_position', 'flux_err_nan_position']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extracting Spectrum from \"g235m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|██████████| 6590/6590 [00:08<00:00, 784.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your FITS files with 1D spectra\n",
    "fits_dir = '/Users/aryanahaghjoo/Documents/GitHub/super_resolution/data/JADES/JADES_spectra_unzipped/hlsp_jades_jwst_nirspec_f170lp-g235m'\n",
    "# Collect both lowercase and uppercase file extensions\n",
    "fits_files = glob.glob(os.path.join(fits_dir, '*.fits')) + glob.glob(os.path.join(fits_dir, '*.FITS'))\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in tqdm(fits_files, desc=\"Processing FITS files\"):\n",
    "    with fits.open(file) as hdul:\n",
    "        # Search for the EXTRACT1D extension\n",
    "        extract1d_hdu = None\n",
    "        for hdu in hdul:\n",
    "            if hdu.header.get('EXTNAME', '').strip().upper() == 'EXTRACT1D':\n",
    "                extract1d_hdu = hdu\n",
    "                break\n",
    "\n",
    "        # Skip files that are not 1D spectra\n",
    "        if extract1d_hdu is None:\n",
    "            continue\n",
    "\n",
    "        # Confirm the extension is a table with columns\n",
    "        if not hasattr(extract1d_hdu, 'columns'):\n",
    "            print(f\"Warning: 'EXTRACT1D' in {os.path.basename(file)} is not a table. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        table_data = extract1d_hdu.data\n",
    "        available_columns = [col.upper() for col in extract1d_hdu.columns.names]\n",
    "        required_columns = ['WAVELENGTH', 'FLUX', 'FLUX_ERR']\n",
    "        if not all(col in available_columns for col in required_columns):\n",
    "            print(f\"Warning: Missing required columns in {os.path.basename(file)}. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        # Extract data columns\n",
    "        wavelength = table_data['WAVELENGTH']\n",
    "        flux = table_data['FLUX']\n",
    "        flux_err = table_data['FLUX_ERR']\n",
    "\n",
    "        data_list.append({\n",
    "            'file_name': os.path.basename(file),\n",
    "            'WAVELENGTH': wavelength,\n",
    "            'FLUX': flux,\n",
    "            'FLUX_ERR': flux_err\n",
    "        })\n",
    "\n",
    "# Create a DataFrame where each row corresponds to a file\n",
    "df_g235m= pd.DataFrame(data_list)\n",
    "#df_g235m.to_csv('/Users/aryanahaghjoo/Documents/GitHub/super_resolution/toy_model/JADES_spectra_dataframe/g235m.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs in FLUX: 273275\n",
      "Total NaNs in FLUX_ERR: 273275\n",
      "NaN positions in FLUX:\n",
      "flux_nan_position\n",
      "middle_nans      3033\n",
      "trailing_nans     237\n",
      "no_nans            19\n",
      "all_nans            6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NaN positions in FLUX_ERR:\n",
      "flux_err_nan_position\n",
      "middle_nans      3033\n",
      "trailing_nans     237\n",
      "no_nans            19\n",
      "all_nans            6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count NaN entries in each file's FLUX and FLUX_ERR arrays\n",
    "df_g235m['nan_in_flux'] = df_g235m['FLUX'].apply(lambda arr: np.isnan(arr).sum())\n",
    "df_g235m['nan_in_flux_err'] = df_g235m['FLUX_ERR'].apply(lambda arr: np.isnan(arr).sum())\n",
    "\n",
    "# Display the count per file\n",
    "#print(df_prism[['file_name', 'nan_in_flux', 'nan_in_flux_err']])\n",
    "\n",
    "# Sum across all files to get the total number of NaN entries\n",
    "total_nan_flux = df_g235m['nan_in_flux'].sum()\n",
    "total_nan_flux_err = df_g235m['nan_in_flux_err'].sum()\n",
    "\n",
    "print(\"Total NaNs in FLUX:\", total_nan_flux)\n",
    "print(\"Total NaNs in FLUX_ERR:\", total_nan_flux_err)\n",
    "\n",
    "# Apply the classification to your DataFrame columns:\n",
    "df_g235m['flux_nan_position'] = df_g235m['FLUX'].apply(classify_nan_positions)\n",
    "df_g235m['flux_err_nan_position'] = df_g235m['FLUX_ERR'].apply(classify_nan_positions)\n",
    "\n",
    "# Optionally, count the occurrences in each category:\n",
    "flux_counts = df_g235m['flux_nan_position'].value_counts()\n",
    "flux_err_counts = df_g235m['flux_err_nan_position'].value_counts()\n",
    "\n",
    "print(\"NaN positions in FLUX:\")\n",
    "print(flux_counts)\n",
    "print(\"\\nNaN positions in FLUX_ERR:\")\n",
    "print(flux_err_counts)\n",
    "\n",
    "# You can also separate the files based on classification, for example:\n",
    "df_g235m_middle = df_g235m[(df_g235m['flux_nan_position'] == 'middle_nans') | (df_g235m['flux_err_nan_position'] == 'middle_nans')]\n",
    "df_g235m_trailing = df_g235m[(df_g235m['flux_nan_position'] == 'trailing_nans') & (df_g235m['flux_err_nan_position'] == 'trailing_nans')]\n",
    "#print(\"\\nFiles with NaNs in the middle:\")\n",
    "#print(df_prism_middle[['file_name', 'flux_nan_position', 'flux_err_nan_position']])\n",
    "#print(\"\\nFiles with only trailing NaNs:\")\n",
    "#print(df_prism_trailing[['file_name', 'flux_nan_position', 'flux_err_nan_position']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting Spectrum from \"g395m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|██████████| 6968/6968 [00:07<00:00, 883.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your FITS files with 1D spectra\n",
    "fits_dir = '/Users/aryanahaghjoo/Documents/GitHub/super_resolution/data/JADES/JADES_spectra_unzipped/hlsp_jades_jwst_nirspec_f290lp-g395m'\n",
    "# Collect both lowercase and uppercase file extensions\n",
    "fits_files = glob.glob(os.path.join(fits_dir, '*.fits')) + glob.glob(os.path.join(fits_dir, '*.FITS'))\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for file in tqdm(fits_files, desc=\"Processing FITS files\"):\n",
    "    with fits.open(file) as hdul:\n",
    "        # Search for the EXTRACT1D extension\n",
    "        extract1d_hdu = None\n",
    "        for hdu in hdul:\n",
    "            if hdu.header.get('EXTNAME', '').strip().upper() == 'EXTRACT1D':\n",
    "                extract1d_hdu = hdu\n",
    "                break\n",
    "\n",
    "        # Skip files that are not 1D spectra\n",
    "        if extract1d_hdu is None:\n",
    "            continue\n",
    "\n",
    "        # Confirm the extension is a table with columns\n",
    "        if not hasattr(extract1d_hdu, 'columns'):\n",
    "            print(f\"Warning: 'EXTRACT1D' in {os.path.basename(file)} is not a table. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        table_data = extract1d_hdu.data\n",
    "        available_columns = [col.upper() for col in extract1d_hdu.columns.names]\n",
    "        required_columns = ['WAVELENGTH', 'FLUX', 'FLUX_ERR']\n",
    "        if not all(col in available_columns for col in required_columns):\n",
    "            print(f\"Warning: Missing required columns in {os.path.basename(file)}. Skipping file.\")\n",
    "            continue\n",
    "\n",
    "        # Extract data columns\n",
    "        wavelength = table_data['WAVELENGTH']\n",
    "        flux = table_data['FLUX']\n",
    "        flux_err = table_data['FLUX_ERR']\n",
    "\n",
    "        data_list.append({\n",
    "            'file_name': os.path.basename(file),\n",
    "            'WAVELENGTH': wavelength,\n",
    "            'FLUX': flux,\n",
    "            'FLUX_ERR': flux_err\n",
    "        })\n",
    "\n",
    "# Create a DataFrame where each row corresponds to a file\n",
    "df_g395m= pd.DataFrame(data_list)\n",
    "#df_g235m.to_csv('/Users/aryanahaghjoo/Documents/GitHub/super_resolution/toy_model/JADES_spectra_dataframe/g395m.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaNs in FLUX: 271025\n",
      "Total NaNs in FLUX_ERR: 271025\n",
      "NaN positions in FLUX:\n",
      "flux_nan_position\n",
      "middle_nans    3473\n",
      "all_nans         11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "NaN positions in FLUX_ERR:\n",
      "flux_err_nan_position\n",
      "middle_nans    3473\n",
      "all_nans         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count NaN entries in each file's FLUX and FLUX_ERR arrays\n",
    "df_g395m['nan_in_flux'] = df_g395m['FLUX'].apply(lambda arr: np.isnan(arr).sum())\n",
    "df_g395m['nan_in_flux_err'] = df_g395m['FLUX_ERR'].apply(lambda arr: np.isnan(arr).sum())\n",
    "\n",
    "# Display the count per file\n",
    "#print(df_prism[['file_name', 'nan_in_flux', 'nan_in_flux_err']])\n",
    "\n",
    "# Sum across all files to get the total number of NaN entries\n",
    "total_nan_flux = df_g395m['nan_in_flux'].sum()\n",
    "total_nan_flux_err = df_g395m['nan_in_flux_err'].sum()\n",
    "\n",
    "print(\"Total NaNs in FLUX:\", total_nan_flux)\n",
    "print(\"Total NaNs in FLUX_ERR:\", total_nan_flux_err)\n",
    "\n",
    "# Apply the classification to your DataFrame columns:\n",
    "df_g395m['flux_nan_position'] = df_g395m['FLUX'].apply(classify_nan_positions)\n",
    "df_g395m['flux_err_nan_position'] = df_g395m['FLUX_ERR'].apply(classify_nan_positions)\n",
    "\n",
    "# Optionally, count the occurrences in each category:\n",
    "flux_counts = df_g395m['flux_nan_position'].value_counts()\n",
    "flux_err_counts = df_g395m['flux_err_nan_position'].value_counts()\n",
    "\n",
    "print(\"NaN positions in FLUX:\")\n",
    "print(flux_counts)\n",
    "print(\"\\nNaN positions in FLUX_ERR:\")\n",
    "print(flux_err_counts)\n",
    "\n",
    "# You can also separate the files based on classification, for example:\n",
    "df_g395m_middle = df_g395m[(df_g395m['flux_nan_position'] == 'middle_nans') | (df_g395m['flux_err_nan_position'] == 'middle_nans')]\n",
    "df_g395m_trailing = df_g395m[(df_g395m['flux_nan_position'] == 'trailing_nans') & (df_g395m['flux_err_nan_position'] == 'trailing_nans')]\n",
    "\n",
    "#print(\"\\nFiles with NaNs in the middle:\")\n",
    "#print(df_prism_middle[['file_name', 'flux_nan_position', 'flux_err_nan_position']])\n",
    "#print(\"\\nFiles with only trailing NaNs:\")\n",
    "#print(df_prism_trailing[['file_name', 'flux_nan_position', 'flux_err_nan_position']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_reso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
